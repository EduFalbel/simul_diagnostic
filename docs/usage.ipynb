{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import matsim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage example notebook\n",
    "\n",
    "In this notebook we'll take a look at some typical workflows that can be crafted using this library. We'll start by taking a look at and using some of the more basic components, then learn how to leverage the provided convenience methods and classes in order to automate a part of the process before finally diving into crafting our own custom classes to extend the library's functionality."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic usage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data prep\n",
    "\n",
    "Since our goal is to compare the simulation against observed data, we must load both of those datasets, as well as a way to know exactly which element of one set should be considered to an element of the other set. In our case, this means providing something like a lookup table relating the links in the simulation's network to the city's detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path.cwd() / './data/zurich'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors_filename = data_folder / 'counts.csv'\n",
    "events_filename = data_folder / 'events.csv'\n",
    "lookup_table_filename = data_folder / 'lookup_table.csv'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll import the data from the loop detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_data = pd.read_csv(detectors_filename, engine='pyarrow')[[\"MSID\", \"MessungDatZeit\", \"AnzFahrzeuge\"]].rename(columns={\"MessungDatZeit\": \"time\", \"AnzFahrzeuge\": \"count\"})\n",
    "detector_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the MATSim events file is unprocessed, we can run the following cells to extract the relevant events and save the result to a csv (which we'll subsequently read)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unprocessed_events_filename = data_folder / 'output_events.xml.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "# import subprocess\n",
    "# subprocess.Popen([utils.get_entered_link_filename, unprocessed_events_filename, data_folder / 'processed_events.csv']).wait()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now import the processed MATSim events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = pd.read_csv(events_filename, engine='c', nrows=10_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df['count'] = 1\n",
    "events_df.rename(columns={'link': 'link_id'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded our base datasets, we'll load the lookup table which will allows us to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table = pd.read_csv(lookup_table_filename, index_col=0)\n",
    "lookup_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Analysis\n",
    "\n",
    "The core of this library is made up of Analysis objects. These objects' sole purpose is to generate their respective analysis (duh) for a given input - a pandas dataframe with specific column names or something that inherits from that (like a geodataframe). In order to use one of these objects, you can simply instantiate them and then call their generate_analysis method while providing the specified dataframe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's do this for the CountComparison Analysis object. As we'll see, what this Analysis does is calculate some link-based metrics comparing the simulated and observed counts. We'll start by reading in an appropriate dataframe. In this case, the dataframe needs to have at least the following columns:\n",
    "\n",
    "* link_id: integer that identifies the individual link\n",
    "* count_sim: the simulated vehicle counts for that link\n",
    "* count_obs: the observed vehicle counts for that link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_data = detector_data.merge(lookup_table, on=\"MSID\", how='right')\n",
    "detector_data = detector_data.astype({'link_id': 'int64'})\n",
    "detector_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diagnostic.report import CreateComparisonDF\n",
    "\n",
    "comparison = CreateComparisonDF.link_comp(events_df, detector_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiating the object\n",
    "\n",
    "In order to create/instantiate a basic Analysis object, all you need to do is import the desired class and call it as though it were a regular python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diagnostic.analyses import CountComparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CountComparison()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating the analysis\n",
    "\n",
    "To actually generate the analysis, simply call the _generate\\_analysis_ method from the Analysis object while providing an appropriate dataframe as the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.generate_analysis(comparison)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to inspect the result of the analysis, we must simply get the 'result' attribute from our Analysis object, as follows. Keep in mind that, depending on the Analysis, this object can be of different types (a pandas DataFrame for CountComparison but a list of matplotlib Figures for CountVisualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the result in a specific format\n",
    "\n",
    "You can also output the generated result in a specific format (csv, latex, png, shp) depending on the analysis being used. To do that, call the object's _to\\_\\<format\\>_ method. For example, to get the result from the CountComparison Analysis as a latex table, we execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.to_latex()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summing up\n",
    "\n",
    "There are many default analyses already implemented, and the process of creating one, generating its analysis, and getting back the result is the same for each one of them. The only thing you must keep in mind when calling the _generate\\_analysis_ method on these objects, however, is that the dataframe column requirements can vary between them. For example, besides the columns already mentioned for the CountComparison analysis, the CountVisualization analysis also requires that the input be a geodataframe from the geopandas library (meaning it should have an active geometry column) in order for the generated plots to make any sense. The required columns are all listed in the respective object's docstring, so if your're unsure all you need to is read it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Going beyond\n",
    "\n",
    "Besides this most elementary use, Analysis objects also support being passed two types of objects when being instantiated: Filters and Options. Each of these objects have a specific purpose:\n",
    "\n",
    "* Options: determine what is computed (such as what statistics)\n",
    "* Filters: determine what is saved to the results attribute (such as what values)\n",
    "\n",
    "By default, each different type of Analysis is instantiated with a specific Options object and the 'identity' Filter (meaning nothing is filtered), however that is easily changed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to only keep the 10 largest entries/links in terms of their calculated SQV and GEH when generating the CountComparison analysis. We can then use one of the already implemented Filter classes in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diagnostic.analyses import FilterByLargest\n",
    "\n",
    "# We first instantiate the filter\n",
    "sqv_10 = FilterByLargest((10, ['SQV', 'GEH']))\n",
    "\n",
    "# Then apply to the Analysis object upon creation\n",
    "cc = CountComparison(sqv_10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, when we generate the analysis and access the result attribute, we will find there are only 10 entries in the DataFrame, those with the highest SQV and GEH values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.generate_analysis(comparison)\n",
    "cc.result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated report generation\n",
    "\n",
    "Manually instantiating each desired Analysis, then calling their respective _generate\\_analysis_ and _to\\_\\<format\\>_, and then joining all those outputs together every time can be a bit of a chore. For this reason we also provide a Report class which automates most of this stuff for you.\n",
    "\n",
    "Suppose, for example, we want to generate a latex document with the CountComparison, CountSummaryStats, and CountVisualization analyses, all neatly formatted and divided into their own sections. Doing that is very straightforward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the desired analyses\n",
    "\n",
    "First, we'll instantiate the analyses we want and add them to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diagnostic.analyses import CountComparison, CountSummaryStats, CountVisualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CountComparison()\n",
    "cs = CountSummaryStats()\n",
    "cv = CountVisualization()\n",
    "\n",
    "analyses = [cc, cs, cv]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding geometry to events\n",
    "\n",
    "Since we want to generate the CountVisualization Analysis, which relies on the passed object being a `GeoDataFrame`, we must add the link geometries to our events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = gpd.read_file(data_folder / 'network/network.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_df = gpd.GeoDataFrame(events_df.merge(network, on='link_id'))\n",
    "events_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating the Report object\n",
    "\n",
    "Second, we'll create an instance of the Report class and feed it our list of analyses as well as two dataframes: the simulated and observed dataframes. These dataframes should have the same columns as one produced by this module's parsers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diagnostic.report import Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_count_report = Report('Link count report', analyses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the analyses\n",
    "\n",
    "Just like we did with the individual Analysis objects, in order to get the Report to generate all of our analyses is to call the `generate_analysis()` method on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_count_report.generate_analyses(events_df, detector_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing individual results\n",
    "\n",
    "Accessing the analyses' individual results when they have been generated through the Report is done exactly the same way as before. Just call the _result_ attribute on the desired Analysis object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Access directly through the object: {cc.result}\")\n",
    "print(f\"Access directly through the object: {cs.result}\")\n",
    "print(f\"Access directly through the object: {cv.result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analysis in analyses:\n",
    "    print(f\"Access through the list defined earlier: {analysis.result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for analysis in link_count_report.analyses:\n",
    "    print(f\"Access through the report's analyses attribute: {analysis.result}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the output file\n",
    "\n",
    "To create our aforementioned latex report, we call the Report object's _to\\_latex_ method and pass in where to save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_count_report.to_latex('/path')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going Beyond\n",
    "\n",
    "#### Specifying analysis dependence\n",
    "\n",
    "Suppose we want the result from one of our analyses, say the CountComparison one, to be fed as the input to some other analysis, CountSummaryStats, automatically as we're generating a report. In order to do that, we can pass in an aditional argument to our Report object when instantiating it called 'analysis_dependence_dict'. This is a dictionary in which the key is the dependent analysis and the value is the analysis on which the previous one depends. So for this given example, we would have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = CountComparison()\n",
    "cs = CountSummaryStats()\n",
    "\n",
    "analyses = [cc, cs]\n",
    "\n",
    "# This is the analysis dependence dictionary\n",
    "add = {cs: cc}\n",
    "\n",
    "report = Report('title', analyses, add)\n",
    "report.generate_analyses(events_df, detector_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, if we take a look at _cs_'s result attribute, we should see that it has a lot more columns than the one we previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending the library\n",
    "\n",
    "It should be fairly easy to add new capabilities to the library, mainly in the form of new analyses, options, and filters, while keeping everything compatible with the higher level convenience objects and methods (such as the Report class).\n",
    "\n",
    "Implementing a new Analysis is as simple as creating a class that extends/inherits from Analysis and defines at least the following method:\n",
    "\n",
    "* _generate\\_analysis_(self, comp: pd.DataFrame)\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diagnostic.analyses import Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyAnalysis(Analysis):\n",
    "    def generate_analysis(self, comp: pd.DataFrame):\n",
    "        # Our analysis is halving the given input\n",
    "        comp = comp/2\n",
    "        self._save_result(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc.result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
